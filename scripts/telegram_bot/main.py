import json
import os
import logging
from telegram import (
    Update,
    InlineQueryResultArticle,
    InputTextMessageContent,
    InlineKeyboardButton,
    InlineKeyboardMarkup
)
from telegram.ext import (
    Application,
    CommandHandler,
    MessageHandler,
    InlineQueryHandler,
    filters,
    CallbackContext,
)

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è ===
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.FileHandler("bot.log"), logging.StreamHandler()],
)
logger = logging.getLogger(__name__)

# === –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ ===
try:
    with open("config.json", "r") as config_file:
        config = json.load(config_file)
        TOKEN = config.get("TOKEN", "")
        if not TOKEN:
            raise ValueError("–¢–æ–∫–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ config.json")
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞ (—Å–æ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
        search_config = config.get("search", {})
        FUZZY_ENABLED = search_config.get("fuzzy_enabled", True)
        MIN_QUERY_LENGTH = search_config.get("min_query_length", 2)
        MAX_SUGGESTIONS = search_config.get("max_suggestions", 28)
        FUZZY_MIN_SCORE = search_config.get("fuzzy_min_score", 70)
        NORMALIZE_DIGRAPHS = search_config.get("normalize_digraphs", True)
        NORMALIZE_DOUBLE_CONSONANTS = search_config.get("normalize_double_consonants", True)

except Exception as e:
    logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥–∞: {e}")
    raise

# === –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä—è ===
def load_words():
    try:
        path = os.path.join("assets", "sutta_words.txt")
        with open(path, "r", encoding="utf-8") as f:
            words = [line.strip() for line in f if line.strip()]
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(words)} —Å–ª–æ–≤ –¥–ª—è –∞–≤—Ç–æ–∫–æ–º–ø–ª–∏—Ç–∞")
            return words
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–ª–æ–≤–∞—Ä—è: {e}")
        return []
 
WORDS = load_words()

# === –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ ===
def normalize(text: str) -> str:
    """–£–º–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ñ–∏–≥–∞"""
    text = text.lower()
    
    # –ë–∞–∑–æ–≤—ã–µ –∑–∞–º–µ–Ω—ã (–≤—Å–µ–≥–¥–∞ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è)
    base_replacements = {
        "·πÅ": "m", "·πÉ": "m", "·π≠": "t", "·∏ç": "d", "·πá": "n",
        "·πÖ": "n", "√±": "n", "ƒÅ": "a", "ƒ´": "i", "≈´": "u"
    }
    
    # –ó–∞–º–µ–Ω—ã –¥–∏–≥—Ä–∞—Ñ–æ–≤ (ph ‚Üí p –∏ —Ç.–¥.)
    digraph_replacements = {
        "ph": "p", "th": "t", "dh": "d", "gh": "g",
        "bh": "b", "jh": "j", "kh": "k", "ch": "c"
    } if NORMALIZE_DIGRAPHS else {}
    
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∑–∞–º–µ–Ω—ã –≤ –æ–¥–∏–Ω —Å–ª–æ–≤–∞—Ä—å
    all_replacements = {**base_replacements, **digraph_replacements}
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –∑–∞–º–µ–Ω—ã
    for old, new in all_replacements.items():
        text = text.replace(old, new)
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–≤–æ–π–Ω—ã—Ö —Å–æ–≥–ª–∞—Å–Ω—ã—Ö
    if NORMALIZE_DOUBLE_CONSONANTS:
        for consonant in "kkgghhcjj·π≠·π≠·∏ç·∏çttddppbbmmnnyyrrlvss":
            text = text.replace(consonant * 2, consonant)
    
    return text

# === –ê–≤—Ç–æ–∫–æ–º–ø–ª–∏—Ç ===
def autocomplete(query: str) -> list[str]:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ñ–∏–≥–∞"""
    if len(query) < MIN_QUERY_LENGTH:
        return []
    
    query_norm = cached_normalize(query)
    exact_matches = [w for w in WORDS if cached_normalize(w).startswith(query_norm)]
    
    # –ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫
    if FUZZY_ENABLED or len(exact_matches) < MAX_SUGGESTIONS // 2:
        # –ü–æ–∏—Å–∫ –ø–æ –ø–æ–¥—Å—Ç—Ä–æ–∫–µ
        substring_matches = [w for w in WORDS if query_norm in cached_normalize(w)]
        exact_matches.extend(substring_matches)
        
        # –§–∞–∑–∑–∏-–ø–æ–∏—Å–∫ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –º–∞–ª–æ)
        if FUZZY_ENABLED and len(exact_matches) < MAX_SUGGESTIONS:
            fuzzy_results = fuzzy_search(query, WORDS, limit=MAX_SUGGESTIONS, score_cutoff=FUZZY_MIN_SCORE)
            exact_matches.extend(fuzzy_results)
    
    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
    results = list(dict.fromkeys(exact_matches))
    results.sort(key=lambda x: (
        not cached_normalize(x).startswith(query_norm),
        len(x)
    ))
    
    return results[:MAX_SUGGESTIONS]

# === –°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∞–≤–∏–∞—Ç—É—Ä—ã —Å –∫–Ω–æ–ø–∫–∞–º–∏ ===
def create_keyboard(query: str) -> InlineKeyboardMarkup:
    search_url = f"https://dhamma.gift/ru/?p=-kn&q={query.replace(' ', '+')}"
    dict_url = f"https://dict.dhamma.gift/ru/search_html?q={query.replace(' ', '+')}"
    
    return InlineKeyboardMarkup([
        [
            InlineKeyboardButton(text="üîé Dhamma.gift", url=search_url),
            InlineKeyboardButton(text="üìö –°–ª–æ–≤–∞—Ä—å", url=dict_url)
        ]
    ])

# === –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –∫–æ–º–∞–Ω–¥ ===
async def start(update: Update, context: CallbackContext):
    user = update.effective_user
    logger.info(f"–ö–æ–º–∞–Ω–¥–∞ /start –æ—Ç {user.id} ({user.full_name})")
    await update.message.reply_text(
        "–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å! –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ:\n"
        "‚Ä¢ –ü—Ä—è–º—ã–µ –∑–∞–ø—Ä–æ—Å—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'sn12.2' –∏–ª–∏ 'dukkha')\n"
        "‚Ä¢ –î–ª—è –ø–æ–¥—Å–∫–∞–∑–æ–∫ –≤ –ª—é–±–æ–º —á–∞—Ç–µ: @dhammagift_bot —Å–ª–æ–≤–æ"
    )


# === –ò–Ω–ª–∞–π–Ω-—Ä–µ–∂–∏–º ===
async def inline_query(update: Update, context: CallbackContext):
    query = update.inline_query.query.strip()
    if not query:
        return

    logger.info(f"–ò–Ω–ª–∞–π–Ω-–∑–∞–ø—Ä–æ—Å: '{query}'")
    suggestions = autocomplete(query)  # –¢–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
    
    results = []
    for idx, word in enumerate(suggestions):
        keyboard = create_keyboard(word)
        
        results.append(
            InlineQueryResultArticle(
                id=f"{word}_{idx}",  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–π id
                title=word,
                input_message_content=InputTextMessageContent(word),
                description=f"–ù–∞–∂–º–∏—Ç–µ, —á—Ç–æ–±—ã –æ—Ç–ø—Ä–∞–≤–∏—Ç—å '{word}'",
                reply_markup=keyboard
            )
        )

    if not results:
        return

    logger.debug(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {[r.id for r in results]}")
    await update.inline_query.answer(results, cache_time=10)

# === –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±—ã—á–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π ===
async def handle_message(update: Update, context: CallbackContext):
    text = update.message.text.strip()
    user = update.effective_user
    logger.info(f"–°–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç {user.id}: {text}")

    # –ê–≤—Ç–æ–∫–æ–º–ø–ª–∏—Ç –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–ª–æ–≤
    if len(text) < 5 and text.isalpha():
        suggestions = autocomplete(text)
        if suggestions:
            reply = "–í–æ–∑–º–æ–∂–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã:\n" + "\n".join(f"- {w}" for w in suggestions)
            await update.message.reply_text(reply)
            return

    # –í—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è —Ç–µ–ø–µ—Ä—å —Å –∫–Ω–æ–ø–∫–∞–º–∏
    keyboard = create_keyboard(text)
    await update.message.reply_text(
        text,
        reply_markup=keyboard
    )

# === –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞ ===
def main():
    logger.info("–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞...")
    try:
        app = Application.builder().token(TOKEN).build()

        # –ö–æ–º–∞–Ω–¥—ã
        app.add_handler(CommandHandler("start", start))

        # –ò–Ω–ª–∞–π–Ω-—Ä–µ–∂–∏–º
        app.add_handler(InlineQueryHandler(inline_query))

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±—ã—á–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
        app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))

        logger.info("–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω")
        app.run_polling()
    except Exception as e:
        logger.critical(f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞: {e}")

if __name__ == "__main__":
    main()