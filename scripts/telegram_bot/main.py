import json
import os
import logging
from functools import lru_cache
from rapidfuzz import fuzz, process
from telegram import (
    Update,
    InlineQueryResultArticle,
    InputTextMessageContent,
    InlineKeyboardButton,
    InlineKeyboardMarkup
)
from telegram.ext import (
    Application,
    CommandHandler,
    MessageHandler,
    InlineQueryHandler,
    filters,
    CallbackContext,
)

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è ===
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.FileHandler("bot.log"), logging.StreamHandler()],
)
logger = logging.getLogger(__name__)

# === –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ ===
try:
    with open("config.json", "r") as config_file:
        config = json.load(config_file)
        TOKEN = config.get("TOKEN", "")
        if not TOKEN:
            raise ValueError("–¢–æ–∫–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ config.json")
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞
        search_config = config.get("search", {})
        FUZZY_ENABLED = search_config.get("fuzzy_enabled", True)
        MIN_QUERY_LENGTH = search_config.get("min_query_length", 2)
        MAX_SUGGESTIONS = search_config.get("max_suggestions", 28)
        FUZZY_MIN_SCORE = search_config.get("fuzzy_min_score", 70)
        NORMALIZE_DIGRAPHS = search_config.get("normalize_digraphs", True)
        NORMALIZE_DOUBLE_CONSONANTS = search_config.get("normalize_double_consonants", True)
        CACHE_SIZE = search_config.get("cache_size", 100_000)

except Exception as e:
    logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥–∞: {e}")
    raise

# === –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä—è ===
def load_words():
    try:
        path = os.path.join("assets", "sutta_words.txt")
        with open(path, "r", encoding="utf-8") as f:
            words = [line.strip() for line in f if line.strip()]
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(words)} —Å–ª–æ–≤")
            return words
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–ª–æ–≤–∞—Ä—è: {e}")
        return []

WORDS = load_words()

# === –£–ª—É—á—à–µ–Ω–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è ===
def get_normalization_table():
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–∞–±–ª–∏—Ü—É –∑–∞–º–µ–Ω –¥–ª—è str.translate()"""
    replacements = {
        '·πÅ': 'm', '·πÉ': 'm', '·π≠': 't', '·∏ç': 'd', '·πá': 'n',
        '·πÖ': 'n', '√±': 'n', 'ƒÅ': 'a', 'ƒ´': 'i', '≈´': 'u'
    }
    if NORMALIZE_DIGRAPHS:
        replacements.update({
            'ph': 'p', 'th': 't', 'dh': 'd', 'gh': 'g',
            'bh': 'b', 'jh': 'j', 'kh': 'k', 'ch': 'c'
        })
    return str.maketrans(replacements)

NORM_TABLE = get_normalization_table()

def normalize(text: str) -> str:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å str.translate()"""
    text = text.lower().translate(NORM_TABLE)
    if NORMALIZE_DOUBLE_CONSONANTS:
        for cons in "kgcj·π≠·∏çtdpbmnrlvs":
            text = text.replace(cons * 2, cons)
    return text

@lru_cache(maxsize=CACHE_SIZE)
def cached_normalize(text: str) -> str:
    """–ö–µ—à–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è normalize()"""
    return normalize(text)

# === –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª–æ–≤ ===
NORMALIZED_WORDS = {word: cached_normalize(word) for word in WORDS}

# === –£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–≤—Ç–æ–∫–æ–º–ø–ª–∏—Ç ===
def autocomplete(query: str) -> list[str]:
    """–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å —Ñ–∞–∑–∑–∏-–º–∞—Ç—á–∏–Ω–≥–æ–º"""
    if len(query) < MIN_QUERY_LENGTH:
        return []

    query_norm = cached_normalize(query)
    
    # 1. –¢–æ—á–Ω—ã–π –ø–æ–∏—Å–∫ –≤ –Ω–∞—á–∞–ª–µ —Å–ª–æ–≤–∞
    exact_matches = [
        word for word, norm_word in NORMALIZED_WORDS.items()
        if norm_word.startswith(query_norm)
    ]
    
    # 2. –ü–æ–∏—Å–∫ –ø–æ –ø–æ–¥—Å—Ç—Ä–æ–∫–µ (–µ—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –º–∞–ª–æ)
    if len(exact_matches) < MAX_SUGGESTIONS:
        substring_matches = [
            word for word, norm_word in NORMALIZED_WORDS.items()
            if query_norm in norm_word and word not in exact_matches
        ]
        exact_matches.extend(substring_matches)
    
    # 3. –§–∞–∑–∑–∏-–ø–æ–∏—Å–∫ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –º–∞–ª–æ)
    if FUZZY_ENABLED and len(exact_matches) < MAX_SUGGESTIONS // 2:
        fuzzy_matches = process.extract(
            query,
            WORDS,
            scorer=fuzz.token_sort_ratio,
            limit=MAX_SUGGESTIONS,
            score_cutoff=FUZZY_MIN_SCORE
        )
        exact_matches.extend(word for word, score in fuzzy_matches if word not in exact_matches)
    
    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
    results = list(dict.fromkeys(exact_matches))
    results.sort(key=lambda x: (
        not NORMALIZED_WORDS[x].startswith(query_norm),  # –°–Ω–∞—á–∞–ª–∞ —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        len(x)  # –ö–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ –≤—ã—à–µ
    ))
    
    return results[:MAX_SUGGESTIONS]

# === –°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∞–≤–∏–∞—Ç—É—Ä—ã ===
def create_keyboard(query: str) -> InlineKeyboardMarkup:
    search_url = f"https://dhamma.gift/ru/?p=-kn&q={query.replace(' ', '+')}"
    dict_url = f"https://dict.dhamma.gift/ru/search_html?q={query.replace(' ', '+')}"
    
    return InlineKeyboardMarkup([
        [
            InlineKeyboardButton(text="üîé Dhamma.gift", url=search_url),
            InlineKeyboardButton(text="üìö –°–ª–æ–≤–∞—Ä—å", url=dict_url)
        ]
    ])

# === –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –∫–æ–º–∞–Ω–¥ ===
async def start(update: Update, context: CallbackContext):
    user = update.effective_user
    logger.info(f"–ö–æ–º–∞–Ω–¥–∞ /start –æ—Ç {user.id} ({user.full_name})")
    await update.message.reply_text(
        "–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å! –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ:\n"
        "‚Ä¢ –ü—Ä—è–º—ã–µ –∑–∞–ø—Ä–æ—Å—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'sn12.2' –∏–ª–∏ 'dukkha')\n"
        "‚Ä¢ –î–ª—è –ø–æ–¥—Å–∫–∞–∑–æ–∫ –≤ –ª—é–±–æ–º —á–∞—Ç–µ: @dhammagift_bot —Å–ª–æ–≤–æ"
    )

async def cache_stats(update: Update, context: CallbackContext):
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–µ—à–∞"""
    stats = cached_normalize.cache_info()
    await update.message.reply_text(
        f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–µ—à–∞:\n"
        f"‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {stats.currsize}/{stats.maxsize}\n"
        f"‚Ä¢ –ü–æ–ø–∞–¥–∞–Ω–∏–π (hits): {stats.hits}\n"
        f"‚Ä¢ –ü—Ä–æ–º–∞—Ö–æ–≤ (misses): {stats.misses}\n"
        f"‚Ä¢ –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å: {stats.hits / (stats.hits + stats.misses) * 100:.1f}%"
    )

# === –ò–Ω–ª–∞–π–Ω-—Ä–µ–∂–∏–º ===
async def inline_query(update: Update, context: CallbackContext):
    query = update.inline_query.query.strip()
    if not query:
        return

    logger.info(f"–ò–Ω–ª–∞–π–Ω-–∑–∞–ø—Ä–æ—Å: '{query}' –æ—Ç {update.inline_query.from_user.id}")
    suggestions = autocomplete(query)

    results = []
    for idx, word in enumerate(suggestions):
        keyboard = create_keyboard(word)
        results.append(
            InlineQueryResultArticle(
                id=f"{word}_{idx}",
                title=word,
                input_message_content=InputTextMessageContent(word),
                description=f"–ù–∞–∂–º–∏—Ç–µ, —á—Ç–æ–±—ã –æ—Ç–ø—Ä–∞–≤–∏—Ç—å '{word}'",
                reply_markup=keyboard
            )
        )

    if results:
        await update.inline_query.answer(results, cache_time=10)

# === –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π ===
async def handle_message(update: Update, context: CallbackContext):
    text = update.message.text.strip()
    user = update.effective_user
    logger.info(f"–°–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç {user.id}: {text}")

    # –ê–≤—Ç–æ–∫–æ–º–ø–ª–∏—Ç –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–ª–æ–≤
    if len(text) < 5 and text.isalpha():
        suggestions = autocomplete(text)
        if suggestions:
            reply = "–í–æ–∑–º–æ–∂–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã:\n" + "\n".join(f"- {w}" for w in suggestions[:8])
            await update.message.reply_text(reply)
            return

    # –í—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è —Å –∫–Ω–æ–ø–∫–∞–º–∏
    await update.message.reply_text(
        text,
        reply_markup=create_keyboard(text)
    )

# === –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞ ===
def main():
    logger.info("–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞...")
    try:
        app = Application.builder().token(TOKEN).build()

        # –ö–æ–º–∞–Ω–¥—ã
        app.add_handler(CommandHandler("start", start))
        app.add_handler(CommandHandler("cache_stats", cache_stats))

        # –ò–Ω–ª–∞–π–Ω-—Ä–µ–∂–∏–º
        app.add_handler(InlineQueryHandler(inline_query))

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π
        app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))

        logger.info("–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω")
        app.run_polling()
    except Exception as e:
        logger.critical(f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞: {e}")

if __name__ == "__main__":
    main()